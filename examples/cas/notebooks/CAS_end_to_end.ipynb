{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fb27b941602401d91542211134fc71a",
   "metadata": {},
   "source": [
    "# CAS (Cluster-Aware Severity) — End‑to‑End (data/cas layout)\n",
    "\n",
    "This notebook reproduces the CAS pipeline using the repo layout:\n",
    "\n",
    "```\n",
    "k-diagram/\n",
    "  data/\n",
    "    cas/\n",
    "      raw/\n",
    "      preprocessed/\n",
    "      modeling_results_ok/\n",
    "      outputs/\n",
    "      Readme.md\n",
    "  examples/\n",
    "    cas/\n",
    "      scripts/\n",
    "        prepare_cas_datasets.py\n",
    "        preprocessing_cas_data.py\n",
    "        cas_modeling.py\n",
    "        results_config.py\n",
    "        results_R1.py\n",
    "        results_R2.py\n",
    "        results_R3.py\n",
    "        results_R4.py\n",
    "        results_R5.py\n",
    "        results_R6.py\n",
    "        results_R7.py\n",
    "        results_R8.py\n",
    "        results_R9.py\n",
    "      notebooks/\n",
    "        CAS_end_to_end.ipynb  ← (this file)\n",
    "      Readme.md\n",
    "```\n",
    "\n",
    "Run the notebook top‑to‑bottom. All artifacts live under `data/cas/`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acae54e37e7d407bbb7b55eff062a284",
   "metadata": {},
   "source": [
    "## 0) Paths & setup\n",
    "Resolves the repository root when the notebook is opened from various\n",
    "locations, then ensures `data/cas/` subfolders exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a63283cbaf04dbcab1f6479b197f3a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repo root: F:\\repositories\\k-diagram\n",
      "Scripts dir: F:\\repositories\\k-diagram\\examples\\cas\\scripts\n",
      "Data root: F:\\repositories\\k-diagram\\data\\cas\n",
      " ├─ raw: F:\\repositories\\k-diagram\\data\\cas\\raw\n",
      " ├─ preprocessed: F:\\repositories\\k-diagram\\data\\cas\\preprocessed\n",
      " ├─ modeling_results_ok: F:\\repositories\\k-diagram\\data\\cas\\modeling_results_ok\n",
      " └─ outputs: F:\\repositories\\k-diagram\\data\\cas\\outputs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def find_repo_root(start: Path) -> Path:\n",
    "    markers = (\"pyproject.toml\", \".git\", \"README.md\")\n",
    "    p = start.resolve()\n",
    "    for _ in range(6):\n",
    "        if (\n",
    "            any((p / m).exists() for m in markers)\n",
    "            and (p / \"examples\").exists()\n",
    "        ):\n",
    "            return p\n",
    "        if p.parent == p:\n",
    "            break\n",
    "        p = p.parent\n",
    "    return start.resolve()\n",
    "\n",
    "\n",
    "repo_root = find_repo_root(Path.cwd())\n",
    "DATA = repo_root / \"data\" / \"cas\"\n",
    "RAW = DATA / \"raw\"\n",
    "PRE = DATA / \"preprocessed\"\n",
    "RESULTS = DATA / \"modeling_results_ok\"\n",
    "FIG_OUT = DATA / \"outputs\"\n",
    "SCRIPTS = repo_root / \"examples\" / \"cas\" / \"scripts\"\n",
    "\n",
    "for p in [RAW, PRE, RESULTS, FIG_OUT]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Repo root:\", repo_root)\n",
    "print(\"Scripts dir:\", SCRIPTS)\n",
    "print(\"Data root:\", DATA)\n",
    "print(\" ├─ raw:\", RAW)\n",
    "print(\" ├─ preprocessed:\", PRE)\n",
    "print(\" ├─ modeling_results_ok:\", RESULTS)\n",
    "print(\" └─ outputs:\", FIG_OUT)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd0d8092fe74a7c96281538738b07e2",
   "metadata": {},
   "source": [
    "## 1) (Optional) Install dependencies\n",
    "Uncomment if running in a fresh environment. Use ``tensorflow==2.15`` preferably"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72eea5119410473aa328ad9291626812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure all the dependencies are well installed.\n",
    "# numpy pandas matplotlib  lightgbm statsmodels scikit-learn [ are all Python existing modules\n",
    "# if missing explicitly install them]\n",
    "\n",
    "# !pip install -U pyarrow fastparquet\n",
    "# # Optional (XTFT & backend)\n",
    "# # !pip install fusionlab-learn tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a10285a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify what kernel you’re using\n",
    "\n",
    "import importlib\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "print(\"python:\", sys.executable)\n",
    "print(\"pandas:\", pd.__version__)\n",
    "for mod in (\n",
    "    \"pyarrow\",\n",
    "    \"fastparquet\",\n",
    "    \"numpy\",\n",
    "    \"pandas\",\n",
    "    \"matplotlib\",\n",
    "    \"lightgbm\",\n",
    "    \"statsmodels\",\n",
    "    \"scikit-learn\",\n",
    "):\n",
    "    try:\n",
    "        m = importlib.import_module(mod)\n",
    "        print(mod, \"OK:\", m.__version__)\n",
    "    except Exception as e:\n",
    "        print(mod, \"FAILED ->\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca705d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ) Install into THIS kernel’s env\n",
    "# If either import failed, install with the kernel’s interpreter:\n",
    "# Uncomment below code if the pyarrow and fastparquet not installed in the selected kernel.\n",
    "\n",
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip uninstall -y pandas fastparquet pyarrow scipy scikit-learn lightgbm \n",
    "!{sys.executable} -m pip install -U --no-cache-dir pandas fastparquet pyarrow lightgbm scipy scikit-learn "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edb47106e1a46a883d545849b8ab81b",
   "metadata": {},
   "source": [
    "## 2) Prepare raw → cleaned domain CSV/Parquet\n",
    "Requires `examples/cas/scripts/prepare_cas_datasets.py` with a function\n",
    "`prepare_all(wind_csv, hydro_csv, subs_csv, out_dir=...)`. It writes\n",
    "cleaned domain files into `data/cas/preprocessed/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10185d26023b46108eb7d9f57d49d2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import util as _imp_util\n",
    "\n",
    "\n",
    "def _load_py(path: Path):\n",
    "    spec = _imp_util.spec_from_file_location(path.stem, str(path))\n",
    "    mod = _imp_util.module_from_spec(spec)\n",
    "    assert spec.loader is not None\n",
    "    spec.loader.exec_module(mod)  # type: ignore\n",
    "    return mod\n",
    "\n",
    "\n",
    "prep_path = SCRIPTS / \"prepare_cas_datasets.py\"\n",
    "assert prep_path.exists(), f\"Missing: {prep_path}\"\n",
    "prep = _load_py(prep_path)\n",
    "\n",
    "wind_csv = RAW / \"gefcom_hourly.csv\"  # or your own name\n",
    "hydro_csv = RAW / \"camels_timeseries.csv\"  # or your own name\n",
    "subs_csv = RAW / \"egms_point.csv\"  # or your own name\n",
    "\n",
    "assert wind_csv.exists() and hydro_csv.exists() and subs_csv.exists(), (\n",
    "    \"Place the required CSVs in data/cas/raw/.\"\n",
    ")\n",
    "\n",
    "prepared = prep.prepare_all(wind_csv, hydro_csv, subs_csv, out_dir=PRE)\n",
    "prepared"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8763a12b2bbd4a93a75aff182afb95dc",
   "metadata": {},
   "source": [
    "## 3) Build supervised frames per domain\n",
    "Uses `examples/cas/scripts/preprocessing_cas_data.py` to produce\n",
    "`supervised_long_*` files in `data/cas/preprocessed/` (CSV/Parquet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7623eae2785240b9bd12b16a66d81610",
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_path = SCRIPTS / \"preprocessing_cas_data.py\"\n",
    "assert preproc_path.exists(), f\"Missing: {preproc_path}\"\n",
    "pre = _load_py(preproc_path)\n",
    "\n",
    "prefer_parquet = True\n",
    "\n",
    "w_manifest = pre.preprocess_wind(\n",
    "    PRE / \"wind_clean.csv\",\n",
    "    outdir=PRE,\n",
    "    suffix=\"_wind\",\n",
    "    prefer_parquet=prefer_parquet,\n",
    ")\n",
    "h_manifest = pre.preprocess_hydro(\n",
    "    PRE / \"hydro_clean.csv\",\n",
    "    outdir=PRE,\n",
    "    suffix=\"_hydro\",\n",
    "    prefer_parquet=prefer_parquet,\n",
    ")\n",
    "s_manifest = pre.preprocess_subsidence(\n",
    "    PRE / \"subsidence_clean.csv\",\n",
    "    outdir=PRE,\n",
    "    suffix=\"_subsidence\",\n",
    "    prefer_parquet=prefer_parquet,\n",
    ")\n",
    "w_manifest, h_manifest, s_manifest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdc8c89c7104fffa095e18ddfef8986",
   "metadata": {},
   "source": [
    "## 4) Train & export predictions/metrics to `data/cas/modeling_results_ok/`\n",
    "Requires `examples/cas/scripts/cas_modeling.py` with `run_domain(domain)`.\n",
    "We repoint its path constants (if present) into `data/cas/`, run all\n",
    "domains, and aggregate to `metrics_all_domains.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b118ea5561624da68c537baed56e602f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "mdl_path = SCRIPTS / \"cas_modeling.py\"\n",
    "assert mdl_path.exists(), f\"Missing: {mdl_path}\"\n",
    "mdl = _load_py(mdl_path)\n",
    "\n",
    "# Repoint if the module exposes these globals\n",
    "if hasattr(mdl, \"BASE_DIR\"):\n",
    "    mdl.BASE_DIR = DATA\n",
    "if hasattr(mdl, \"PQT_DIR\"):\n",
    "    mdl.PQT_DIR = PRE\n",
    "if hasattr(mdl, \"CSV_DIR\"):\n",
    "    mdl.CSV_DIR = PRE\n",
    "if hasattr(mdl, \"OUT_DIR\"):\n",
    "    mdl.OUT_DIR = RESULTS\n",
    "\n",
    "RESULTS.mkdir(parents=True, exist_ok=True)\n",
    "all_mets = []\n",
    "for dom in (\"hydro\", \"subsidence\", \"wind\"):\n",
    "    _pred_df, m = mdl.run_domain(dom)\n",
    "    all_mets.append(m)\n",
    "\n",
    "combo = pd.concat(all_mets, ignore_index=True)\n",
    "combo.to_csv(RESULTS / \"metrics_all_domains.csv\", index=False)\n",
    "combo.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938c804e27f84196a10c8828c723f798",
   "metadata": {},
   "source": [
    "## 5) Build paper figures/tables into `data/cas/outputs/`\n",
    "Runs plotting scripts with `BASE_DIR = DATA` so they read from\n",
    "`modeling_results_ok/` and save images/tables under `outputs/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504fb2a444614c0babb325280ed9130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import runpy\n",
    "\n",
    "plots = [SCRIPTS / \"results_R{n+1}.py\" for n in range(10)]\n",
    "plots += [SCRIPTS / \"results_config.py\"]\n",
    "\n",
    "for ps in plots:\n",
    "    assert ps.exists(), f\"Missing plotting script: {ps}\"\n",
    "    runpy.run_path(str(ps), init_globals={\"BASE_DIR\": DATA})\n",
    "\n",
    "sorted(FIG_OUT.glob(\"*\"))[:6]  # preview a few outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bbdb311c014d738909a11f9e486628",
   "metadata": {},
   "source": [
    "## Appendix — README quickstart for this layout\n",
    "Use this in `examples/cas/Readme.md`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43b363d81ae4b689946ece5c682cd59",
   "metadata": {},
   "source": [
    "```bash\n",
    "git clone https://github.com/earthai-tech/k-diagram\n",
    "cd k-diagram\n",
    "\n",
    "mkdir -p data/cas/{raw,preprocessed,modeling_results_ok,outputs}\n",
    "\n",
    "# Place inputs here (or your equivalents)\n",
    "# data/cas/raw/gefcom_hourly.csv\n",
    "# data/cas/raw/camels_timeseries.csv\n",
    "# data/cas/raw/egms_point.csv\n",
    "\n",
    "pip install -U numpy pandas matplotlib pyarrow lightgbm statsmodels scikit-learn\n",
    "# Optional\n",
    "# pip install fusionlab-learn tensorflow\n",
    "\n",
    "jupyter lab  # open examples/cas/notebooks/CAS_end_to_end.ipynb\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (watex)",
   "language": "python",
   "name": "watex"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
